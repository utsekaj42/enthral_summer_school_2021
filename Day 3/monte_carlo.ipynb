{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa40e92e",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html monte_carlo.do.txt --ipynb_admon=hrule --without_solutions --no_abort -->\n",
    "<!-- dom:TITLE: A brief introduction to UQ and SA with the Monte Carlo method -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e7317",
   "metadata": {
    "editable": true
   },
   "source": [
    "# A brief introduction to UQ and SA with the Monte Carlo method\n",
    "**Jacob Sturdy**, NTNU  \n",
    "**Vinzenz Gregor Eck**, Expert Analytics  \n",
    "**Leif Rune Hellevik**, NTNU\n",
    "\n",
    "Date: **Jul 1, 2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "691c0482",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ipython magic\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bedc6d6",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2654cde8",
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chaospy as cp\n",
    "import chaospy_wrapper as cpw\n",
    "import monte_carlo\n",
    "from sensitivity_examples_nonlinear import generate_distributions\n",
    "from sensitivity_examples_nonlinear import monte_carlo_sens_nonlin\n",
    "from sensitivity_examples_nonlinear import analytic_sensitivity_coefficients\n",
    "from sensitivity_examples_nonlinear import polynomial_chaos_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7922f269",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Monte Carlo\n",
    "\n",
    "The Monte Carlo method (MCM)  is probably the most widely applied method for\n",
    "variance based uncertainty quantification and sensitivity\n",
    "analysis. Monte carlo methods are generally straight forward to use\n",
    "and may be applied to a wide variety of problems as they require few\n",
    "assumptions about the model or quantity of interest and require no\n",
    "modifications of the model itself, i.e. the model may be used as a\n",
    "black box. The basic idea is to calculate statistics (mean, standard\n",
    "deviation, variance, sobol indices) of $Y$ directly from large amount\n",
    "of sample evaluations from the black box model $y$.\n",
    "\n",
    "<hr/>\n",
    "**Monte Carlo approach.**\n",
    "\n",
    "1. Sample a set of input samples $\\mathbf{z}^{(s)}$ from the input space $\\Omega_\\mathbf{Z}$ that is defined by the joint probability density function ${F_Z}$.\n",
    "\n",
    "2. Evaluate the deterministic model $y(\\mathbf{z})$ for each sample in $\\mathbf{z}^{(s)}$ to produce a set of model outputs $y^{(s)}$. \n",
    "\n",
    "3. Estimate all uncertainty measures and sensitivity indices from $y^{(s)}$.\n",
    "<hr/>\n",
    "\n",
    "For demonstration purposes we will use the same model as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd71afb1",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# start the linear model\n",
    "def linear_model(w, z):\n",
    "    return np.sum(w*z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba44b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Expectation and variance\n",
    "\n",
    "Once the model outputs have been computed the expectation and variance\n",
    "of the output are computed with the normal estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86431292",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:expected_value_MonteCarlo\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    {\\mathbb{E}}(Y) \\approx \\frac{1}{N} \\sum_{s=1}^{N} y^{(s)} \\qquad \\text{and} \\qquad       \\operatorname{Var}(Y) \\approx \\frac{1}{N\\!-\\!1} \\sum_{s=1}^{N}  \\left( y^{(s)} - {\\mathbb{E}}(Y)\\right)^2.\n",
    "     \\tag{1}\n",
    "  \\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a225da",
   "metadata": {
    "editable": true
   },
   "source": [
    "Below we demonstrate how  `chaospy` may be used for sampling and `numpy` for the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e46fc9b",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    # start uq\n",
    "    # generate the distributions for the problem\n",
    "    Nrv = 4\n",
    "    c = 0.5\n",
    "    zm = np.array([[0., i] for i in range(1, Nrv + 1)])\n",
    "    wm = np.array([[i * c, i] for i in range(1, Nrv + 1)])\n",
    "    jpdf = generate_distributions(zm, wm)\n",
    "\n",
    "    # 1. Generate a set of Xs\n",
    "    Ns = 20000\n",
    "    Xs = jpdf.sample(Ns, rule='R').T  # <- transform the sample matrix\n",
    "\n",
    "    # 2. Evaluate the model\n",
    "    Zs = Xs[:, :Nrv]\n",
    "    Ws = Xs[:, Nrv:]\n",
    "    Ys = linear_model(Ws, Zs)\n",
    "\n",
    "    # 3. Calculate expectation and variance\n",
    "    EY = np.mean(Ys)\n",
    "    VY = np.var(Ys, ddof=1)  # NB: use ddof=1 for unbiased variance estimator, i.e /(Ns - 1)\n",
    "\n",
    "    print('E(Y): {:2.5f} and  Var(Y): {:2.5f}'.format(EY, VY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234c472",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Low Discrepancy Sequences\n",
    "\n",
    "Monte Carlo estimates have the following asymptotic convergence in distribution when sampling a random variable $X$ to estimate its mean:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd7f1c",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sqrt{n}(\\theta_n - E(X)) \\xrightarrow d \\mathcal{N}(0,\\operatorname{Var}(X))\n",
    " \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e707b2a",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\theta_n$ is the sample mean.\n",
    "\n",
    "A Monte Carlo estimator for a quantity $y$ is based on generating a sample of a random variable $X$ and taking its mean as the estimate for $y$. The random variable $X$ is defined in such that the expected value of $X$ is the quantity (often an integral) we are interested in: $E(X) =y$. Note that expectation and integration are effectively the same. Thus we expect the error in our estimate of $E(X)$ to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98392191",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E(X) - y ~ \\sqrt{\\frac{1}{n} \\operatorname{Var}(X)}\n",
    " \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e718780",
   "metadata": {
    "editable": true
   },
   "source": [
    "which you may notice is the standard error of the estimate of the mean of $X$.\n",
    "\n",
    "Thus if we can define $X$ such that $\\operatorname{Var} (X)$ is lower, the convergence may improve. This is generally referred to as variance reduction in Monte Carlo approaches, and a common way of achieving this is to use special sequences of numbers to generate samples of $X$ instead of random sampling. Below we show some samples generated by some of these approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a194269",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# example sampling\n",
    "u1 = cp.Uniform(0,1)\n",
    "u2 = cp.Uniform(0,1)\n",
    "joint_distribution = cp.J(u1, u2)\n",
    "number_of_samples = 350\n",
    "samples_random = joint_distribution.sample(size=number_of_samples, rule='R')\n",
    "# end example sample gen\n",
    "samples_hammersley = joint_distribution.sample(size=number_of_samples, rule='hammersley')\n",
    "samples_sobol = joint_distribution.sample(size=number_of_samples, rule='sobol')\n",
    "samples_lhs = joint_distribution.sample(size=number_of_samples, rule='latin_hypercube')\n",
    "samples_halton = joint_distribution.sample(size=number_of_samples, rule='halton')\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Random')\n",
    "ax1.scatter(*samples_random)\n",
    "ax1.set_xlabel(\"Uniform 1\")\n",
    "ax1.set_ylabel(\"Uniform 2\")\n",
    "ax1.axis('equal')\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Hammersley sampling')\n",
    "ax2.scatter(*samples_hammersley)\n",
    "ax2.set_xlabel(\"Uniform 1\")\n",
    "ax2.set_ylabel(\"Uniform 2\")\n",
    "ax2.axis('equal')\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Sobol sampling')\n",
    "ax2.scatter(*samples_sobol)\n",
    "ax2.set_xlabel(\"Uniform 1\")\n",
    "ax2.set_ylabel(\"Uniform 2\")\n",
    "ax2.axis('equal')\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Latin Hypercube sampling')\n",
    "ax2.scatter(*samples_lhs)\n",
    "ax2.set_xlabel(\"Uniform 1\")\n",
    "ax2.set_ylabel(\"Uniform 2\")\n",
    "_ = ax2.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7980f3",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Variance based sensitivity measures\n",
    "\n",
    "In our [sensitivity_introduction notebook](sensitivity_introduction.ipynb) model we calculated the sensitivity\n",
    "coefficients with the MCM in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bcf129f",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    # sensitivity analytical values\n",
    "    Sa, Szw, Sta = analytic_sensitivity_coefficients(zm, wm)\n",
    "    N_prms = len(jpdf)\n",
    "    N_terms = N_prms//2\n",
    "\n",
    "    # Monte Carlo\n",
    "    Ns_mc = 10000 # Number of samples mc\n",
    "    # calculate sensitivity indices with mc\n",
    "    A_s, B_s, C_s, f_A, f_B, f_C, Smc, Stmc = monte_carlo_sens_nonlin(Ns_mc, jpdf)\n",
    "\n",
    "    # compute with Polynomial Chaos\n",
    "    Ns_pc = 200\n",
    "    polynomial_order = 3\n",
    "    \n",
    "    # calculate sensitivity indices with gpc\n",
    "    Spc, Stpc, gpce_reg = polynomial_chaos_sens(Ns_pc, jpdf, polynomial_order,return_reg=True)\n",
    "\n",
    "    # compare the computations\n",
    "    import pandas as pd\n",
    "    row_labels  = ['X_'+str(x) for x in range(1,N_prms+1)]\n",
    "    S=np.column_stack((Sa,Spc,Smc,Sta,Stpc,Stmc))\n",
    "    S_table = pd.DataFrame(S, columns=['Sa','Spc','Smc','Sta','Stpc','Stmc'], index=row_labels)  \n",
    "    print(S_table.round(3))\n",
    "\n",
    "    # Second order indices with gpc\n",
    "    S2 = cpw.Sens_m2(gpce_reg, jpdf) # second order indices with gpc\n",
    "    \n",
    "    # print all second order indices\n",
    "    print(pd.DataFrame(S2,columns=row_labels,index=row_labels).round(3))\n",
    "    \n",
    "    # sum all second order indices \n",
    "    SumS2=np.sum(np.triu(S2))\n",
    "    print('\\nSum Sij = {:2.2f}'.format(SumS2))\n",
    "    \n",
    "    # sum all first and second order indices\n",
    "    print('Sum Si + Sij = {:2.2f}\\n'.format(np.sum(Spc)+SumS2))\n",
    "    \n",
    "    # compare nonzero second order indices with analytical indices \n",
    "    Szw_pc=[S2[i,i+N_terms] for i in range(N_terms) ]\n",
    "    Szw_table=np.column_stack((Szw_pc,Szw,(Szw_pc-Szw)/Szw))\n",
    "    print(pd.DataFrame(Szw_table,columns=['Szw','Szw pc','Error%']).round(3))\n",
    "    \n",
    "    # end second order\n",
    "    convergence_analysis = False\n",
    "    if convergence_analysis:\n",
    "        # Convergence analysis\n",
    "        # Convergence Monte Carlo with random sampling\n",
    "        list_of_samples = np.array([10000, 50000, 100000, 500000, 1000000])\n",
    "        s_mc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        st_mc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        # average over\n",
    "        N_iter = 5\n",
    "        print('MC convergence analysis:')\n",
    "        for i, N_smpl in enumerate(list_of_samples):\n",
    "            print('    N_smpl {}'.format(N_smpl))\n",
    "            for j in range(N_iter):\n",
    "                A_s, XB, XC, Y_A, Y_B, Y_C, S, ST = monte_carlo_sens_nonlin(N_smpl,\n",
    "                                                                                jpdf,\n",
    "                                                                                rule='R')\n",
    "                s_mc_err[i] += np.abs(S - Sa)\n",
    "                st_mc_err[i] += np.abs(ST - Sta)\n",
    "                print('         finished with iteration {} of {}'.format(1 + j, N_iter))\n",
    "            s_mc_err[i] /= float(N_iter)\n",
    "            st_mc_err[i] /= float(N_iter)\n",
    "        # Plot results for monte carlo\n",
    "        fig_random = plt.figure('Random sampling - average of indices')\n",
    "        fig_random.suptitle('Random sampling - average of indices')\n",
    "\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        plt.title('First order sensitivity indices')\n",
    "        _=plt.plot(list_of_samples / 1000, np.sum(s_mc_err, axis=1), '-')\n",
    "        ax.set_yscale('log')\n",
    "        _=plt.ylabel('abs error')\n",
    "        _=plt.xlabel('number of samples [1e3]')\n",
    "\n",
    "        ax1 = plt.subplot(1, 2, 2)\n",
    "        plt.title('Total sensitivity indices')\n",
    "        _=plt.plot(list_of_samples / 1000, np.sum(st_mc_err, axis=1), '-')\n",
    "        ax1.set_yscale('log')\n",
    "        _=plt.ylabel('abs error')\n",
    "        _=plt.xlabel('number of samples [1e3]')\n",
    "\n",
    "        # Plot results for monte carlo figure individual\n",
    "        fig_random = plt.figure('Random sampling')\n",
    "        fig_random.suptitle('Random sampling')\n",
    "        for l, (s_e, st_e) in enumerate(zip(s_mc_err.T, st_mc_err.T)):\n",
    "            ax = plt.subplot(1, 2, 1)\n",
    "            plt.title('First order sensitivity indices')\n",
    "            plt.plot(list_of_samples / 1000, s_e, '-', label='S_{}'.format(l))\n",
    "            ax.set_yscale('log')\n",
    "            _=plt.ylabel('abs error')\n",
    "            _=plt.xlabel('number of samples [1e3]')\n",
    "            _=plt.legend()\n",
    "\n",
    "            ax1 = plt.subplot(1, 2, 2)\n",
    "            plt.title('Total sensitivity indices')\n",
    "            _=plt.plot(list_of_samples / 1000, st_e, '-', label='ST_{}'.format(l))\n",
    "            ax1.set_yscale('log')\n",
    "            _=plt.ylabel('abs error')\n",
    "            _=plt.xlabel('number of samples [1e3]')\n",
    "            plt.legend()\n",
    "\n",
    "        # Convergence Polynomial Chaos\n",
    "        list_of_samples = np.array([140, 160, 200, 220])\n",
    "        s_pc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        st_pc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        polynomial_order = 3\n",
    "        # average over\n",
    "        N_iter = 4\n",
    "        print('PC convergence analysis:')\n",
    "        basis = cpw.generate_basis(polynomial_order, jpdf)\n",
    "        for i, N_smpl in enumerate(list_of_samples):\n",
    "            print('    N_smpl {}'.format(N_smpl))\n",
    "            for j in range(N_iter):\n",
    "                # calculate sensitivity indices\n",
    "                Spc, Stpc = polynomial_chaos_sens(N_smpl, jpdf, polynomial_order, basis=basis)\n",
    "                s_pc_err[i] += np.abs(Spc - Sa)\n",
    "                st_pc_err[i] += np.abs(Stpc - Sta)\n",
    "                print('         finished with iteration {} of {}'.format(1 + j, N_iter))\n",
    "            s_pc_err[i] /= float(N_iter)\n",
    "            st_pc_err[i] /= float(N_iter)\n",
    "\n",
    "        # Plot results for polynomial chaos\n",
    "        fig_random = plt.figure('Polynomial Chaos - average of indices')\n",
    "        fig_random.suptitle('Polynomial Chaos - average of indices')\n",
    "\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        plt.title('First order sensitivity indices')\n",
    "        _=plt.plot(list_of_samples, np.sum(s_pc_err, axis=1), '-')\n",
    "        ax.set_yscale('log')\n",
    "        _=plt.ylabel('abs error')\n",
    "        _=plt.xlabel('number of samples [1e3]')\n",
    "\n",
    "        ax1 = plt.subplot(1, 2, 2)\n",
    "        plt.title('Total sensitivity indices')\n",
    "        _=plt.plot(list_of_samples, np.sum(st_pc_err, axis=1), '-')\n",
    "        ax1.set_yscale('log')\n",
    "        _=plt.ylabel('abs error')\n",
    "        _=plt.xlabel('number of samples [1e3]')\n",
    "\n",
    "        # Plot results for polynomial chaos individual\n",
    "        fig_random = plt.figure('Polynomial Chaos')\n",
    "        fig_random.suptitle('Polynomial Chaos')\n",
    "        for l, (s_e, st_e) in enumerate(zip(s_pc_err.T, st_pc_err.T)):\n",
    "            ax = plt.subplot(1, 2, 1)\n",
    "            plt.title('First order sensitivity indices')\n",
    "            _=plt.plot(list_of_samples, s_e, '-', label='S_{}'.format(l))\n",
    "            ax.set_yscale('log')\n",
    "            plt.ylabel('abs error')\n",
    "            plt.xlabel('number of samples [1e3]')\n",
    "            plt.legend()\n",
    "\n",
    "            ax1 = plt.subplot(1, 2, 2)\n",
    "            plt.title('Total sensitivity indices')\n",
    "            _=plt.plot(list_of_samples, st_e, '-', label='ST_{}'.format(l))\n",
    "            ax1.set_yscale('log')\n",
    "            plt.ylabel('abs error')\n",
    "            plt.xlabel('number of samples [1e3]')\n",
    "            plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e6e0c",
   "metadata": {
    "editable": true
   },
   "source": [
    "The actual algorithm calculating the sensitivity analysis was hidden in this function call which did the magic for us: `A_s, B_s, C_s, f_A, f_B, f_C, Smc, Stmc = monte_carlo_sens_nonlin(Ns_mc, jpdf)` \n",
    "\n",
    "Below we explain in greater detail Saltelli's algorithm which is used to compute the Sobol indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5943c0",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Saltelli's algorithm for Sobol indices estimation\n",
    "\n",
    "Calculating the sensitivity coefficients with MCM directly is\n",
    "computationally very expensive. To see this, consider how on would\n",
    "estimate $\\operatorname{Var}\\mathbb{E}(Y|Z_i))$ which is the numerator in the Sobol\n",
    "indices, in a direct brute force, manner. Let $M$ be the evaluations\n",
    "needed to estimate the inner, conditional expected value $\\mathbb{E}(Y|Z_i)$\n",
    "for a fixed $Z_i$. To get an approxiamation of the outer variance, one\n",
    "would have to repeat this process for the whole range of $Z_i$, which\n",
    "could also amount to $\\propto M$. Finally, this would have to be done\n",
    "for all $r$ input random variables of $Y$. Consecquently, the number\n",
    "of evalutations amounts to $\\mathcal{O}(M^2 \\;r)$. To get a impression\n",
    "of what this could to, note that in many cases a reasonable $M$ could\n",
    "be $5000$ which would results in $M^2 =25 000 000$ necessary\n",
    "evaluations!\n",
    "\n",
    "Luckily Saltelli came up with an algorithm to approximate of the sensitivity first order coefficients using $M(p+2)$ evaluations in total\n",
    "There are many adaptations and improvements of the algorithm available, here we will present the basic idea of the algorithm.\n",
    "\n",
    "<hr/>\n",
    "**Saltelli's algorithm.**\n",
    "\n",
    "1. Use a sampling method to draw a set of input samples $\\mathbf{z}^{(s)}$\n",
    "\n",
    "2. Evaluate the deterministic model $y(\\mathbf{z})$ for each sample\n",
    "\n",
    "3. Estimate all sensitivity indices from $y^{(s)}$.\n",
    "<hr/>\n",
    "\n",
    "Thus, the blackbox function mentioned above, follows these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4564a92",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# calculate sens indices of non additive model\n",
    "def monte_carlo_sens_nonlin(Ns, jpdf, rule='R'):\n",
    "    N_prms = len(jpdf)\n",
    "\n",
    "    # 1. Generate sample matrices\n",
    "    XA, XB, XC = generate_sample_matrices_mc(Ns, N_prms, jpdf, sample_method=rule)\n",
    "\n",
    "    # 2. Evaluate the model\n",
    "    Y_A, Y_B, Y_C = evaluate_non_additive_linear_model(XA, XB, XC)\n",
    "\n",
    "    # 3. Approximate the sensitivity indices\n",
    "    S, ST = calculate_sensitivity_indices_mc(Y_A, Y_B, Y_C)\n",
    "\n",
    "    return XA, XB, XC, Y_A, Y_B, Y_C, S, ST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427549c2",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Saltelli's algorithm step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b456a",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: sample matrix creation\n",
    "\n",
    "For Saltelli's Algorithm we need to create two different sample matrices $A,B$ each of the size $M\\times P$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11291d0b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{A} =\n",
    "\\begin{bmatrix}\n",
    "z_1^{(A,1)} & \\cdots z_i^{(A,1)} \\cdots & z_P^{(A,1)} \\\\\n",
    "\\vdots &\t\t    & \\vdots \\\\\n",
    "z_i^{(A,M)} & \\cdots z_i^{(A,M)} \\cdots & z_P^{(A,M)}\n",
    "\\end{bmatrix}\n",
    ", \\quad\n",
    "\\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "z_1^{(B,1)} & \\cdots z_i^{(B,1)} \\cdots & z_P^{(B,1)} \\\\\n",
    "\\vdots &\t\t    & \\vdots \\\\\n",
    "z_i^{(B,M)} & \\cdots z_i^{(B,M)} \\cdots & z_P^{(B,M)}\n",
    "\\end{bmatrix}\n",
    ".\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22708b86",
   "metadata": {
    "editable": true
   },
   "source": [
    "In addition we create $P$ additional matrices $C_i$ of the size $M\\times P$ compound of matrix $A$ and matrix $B$. In a matrix $C_i$ all colums will be have the same values as the $B$ matrix, except the $i$-th column, which will have the values of $A$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613c253",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{C}_i =\n",
    "\\begin{bmatrix}\n",
    "z_1^{(B,1)} & \\cdots z_i^{(A,1)} \\cdots & z_P^{(B,1)} \\\\\n",
    "\\vdots &\t\t    & \\vdots \\\\\n",
    "z_i^{(B,M)} & \\cdots z_i^{(A,M)} \\cdots & z_P^{(B,M)}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103bdc82",
   "metadata": {
    "editable": true
   },
   "source": [
    "This was implemented in the method:\n",
    "`A, B, C = generate_sample_matrices_mc(number_of_samples, number_of_parameters, joint_distribution, sample_method)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf31ef5",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sample matrices\n",
    "def generate_sample_matrices_mc(Ns, number_of_parameters, jpdf, sample_method='R'):\n",
    "    \"\"\"\n",
    "    Generate the sample matrices A, B and C_i for Saltelli's algorithm\n",
    "    Inputs:\n",
    "    Ns (int): Number of independent samples for each matrices A and B\n",
    "    jpdf (dist): distribution object with a sample method\n",
    "    sample_method (str): specify sampling method to use\n",
    "    Returns A, B, C\n",
    "    A, B (arrays): samples are rows and parameters are columns\n",
    "    C (arrays): samples are first dimension, second index indicates which parameter is fixed,\n",
    "    and parameters values third are indexed by third dimension\n",
    "    \"\"\"\n",
    "    if not jpdf.stochastic_dependent:\n",
    "        # Fix for Sobol sequence issue where A and B are not\n",
    "        # independent if formed by dividing sequence in two parts\n",
    "        # TODO submit pull request for a better way to accomplish this with chaospy\n",
    "        j2pdf = [eval(\"cp.\"+d.__str__()) for d in jpdf] + [eval(\"cp.\"+d.__str__())for d in jpdf]\n",
    "        j2pdf = cp.J(*j2pdf)\n",
    "        Xtot = j2pdf.sample(Ns, sample_method)\n",
    "        A = Xtot[0:number_of_parameters].T\n",
    "        B = Xtot[number_of_parameters:].T\n",
    "    else:\n",
    "        raise NotImplementedError('You cannot use this method for dependent distributions')\n",
    "\n",
    "    C = np.empty((number_of_parameters, Ns, number_of_parameters))\n",
    "    # create C sample matrices\n",
    "    for i in range(number_of_parameters):\n",
    "        C[i, :, :] = B.copy()\n",
    "        C[i, :, i] = A[:, i].copy()\n",
    "\n",
    "    return A, B, C\n",
    "\n",
    "\n",
    "\n",
    "# mc algorithm for variance based sensitivity coefficients\n",
    "def calculate_sensitivity_indices_mc(y_a, y_b, y_c, main='sobol', total='homma'):\n",
    "    \"\"\"\n",
    "    Sobol's 1993 algorithm for S_m using Monte Carlo integration\n",
    "    Sobol's 2007 algorithm for S_t using Monte Carlo integration\n",
    "    Saltelli's 2010 algorithm for estimating S_m and \n",
    "    Homma's 1996 algorithm for estimating S_t\n",
    "    \n",
    "    Inputs: y_a, y_b (array): first index corresponds to sample second to variables of interest\n",
    "    y_c (array): first index corresponds conditional index, second to sample and \n",
    "        following dimensions to variables of interest\n",
    "        main (str): specify which method from ('sobol', 'saltelli') to use for S_m\n",
    "        total (str): specify which method from ('sobol', 'homma') to use for S_t\n",
    "        \n",
    "    Returns: s, st\n",
    "        s (array): first order sensitivities first index corrresponds to input second \n",
    "            to variable of interest\n",
    "        st (array): total sensitivities first index corrresponds to input second \n",
    "            to variable of interest\n",
    "    \"\"\"\n",
    "    s_shape = y_c.shape[0:1] + y_c.shape[2:]\n",
    "    s = np.zeros(s_shape)\n",
    "    st = np.zeros(s_shape)\n",
    "\n",
    "    mean = 0.5*(np.mean(y_a,axis=0) + np.mean(y_b,axis=0))\n",
    "    y_a_center = y_a - mean\n",
    "    y_b_center = y_b - mean\n",
    "    f0sq = np.mean(y_a_center,axis=0) * np.mean(y_b_center,axis=0) # 0 when data is centered\n",
    "    var_est = np.var(y_b, axis=0)\n",
    "    for i, y_c_i in enumerate(y_c):\n",
    "        y_c_i_center = y_c_i - mean\n",
    "        if main=='sobol':\n",
    "            s[i] = (np.mean(y_a_center*y_c_i_center, axis=0)-f0sq)/var_est \n",
    "        elif main=='saltelli':\n",
    "            s[i] = np.mean(y_a_center*(y_c_i_center - y_b_center), axis=0)/var_est \n",
    "        else:\n",
    "            raise ValueError('Unknown method main=\"%s\"' % main)\n",
    "\n",
    "        if total=='homma':\n",
    "            st[i] = 1 - (np.mean(y_c_i_center*y_b_center, axis=0) - f0sq)/var_est \n",
    "        elif total=='sobol':\n",
    "            st[i] = np.mean(y_b_center*(y_b_center-y_c_i_center), axis=0)/var_est\n",
    "        else:\n",
    "            raise ValueError('Unknown method total=\"%s\"' % total)\n",
    "    return s, st\n",
    "\n",
    "\n",
    "# end mc algorithm for variance based sensitivity coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592084b6",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: evaluate the model for samples\n",
    "\n",
    "In the second step we evaluate the model for samples in the matrices\n",
    "and save the results in vectors $Y_{\\mathbf{A}}$, $Y_{\\mathbf{B}}$ and\n",
    "$Y_{\\mathbf{C_i}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce900d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "Y_{\\mathbf{A}} = y(\\mathbf{A}), \\qquad Y_{\\mathbf{B}} = y(\\mathbf{B}), \\qquad  Y_{\\mathbf{C_i}} = y(\\mathbf{C_i}),\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657ee7a",
   "metadata": {
    "editable": true
   },
   "source": [
    "The corresponding python code for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a33575",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "def evaluate_non_additive_linear_model(X_A, X_B, X_C):\n",
    "\n",
    "    N_prms = X_A.shape[1]\n",
    "    Ns = X_A.shape[0]\n",
    "    N_terms = int(N_prms / 2)\n",
    "    # 1. evaluate sample matrices X_A\n",
    "    Z_A = X_A[:, :N_terms]  # Split X in two vectors for X and W\n",
    "    W_A = X_A[:, N_terms:]\n",
    "    Y_A = linear_model(W_A, Z_A)\n",
    "\n",
    "    # 2. evaluate sample matrices X_B\n",
    "    Z_B = X_B[:, :N_terms]\n",
    "    W_B = X_B[:, N_terms:]\n",
    "    Y_B = linear_model(W_B, Z_B)\n",
    "\n",
    "    # 3. evaluate sample matrices X_C\n",
    "    Y_C = np.empty((N_prms, Ns))\n",
    "    for i in range(N_prms):\n",
    "        x = X_C[i, :, :]\n",
    "        z = x[:, :N_terms]\n",
    "        w = x[:, N_terms:]\n",
    "        Y_C[i,:] = linear_model(w, z)\n",
    "\n",
    "    return Y_A, Y_B, Y_C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c9b37",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: approximate the sensitivity indices\n",
    "\n",
    "In the final step the first order and total Sobol indices are estimated.\n",
    "Since the numerical approximation of all indices are quite demanding, approximations are used to speed up the process.\n",
    "For both, the first and total sensitivity index, exist several approximations, which the most common can be found in ([[saltelli2010]](#saltelli2010))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913ef81",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The first order sensitivity indices\n",
    "\n",
    "The first order indices are defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008f1b1",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "S_i = \\frac{\\operatorname{Var}_{Z_i}(E_{\\mathbf{Z}_{-i}} (Y\\;|\\;Z_i))}{\\operatorname{Var}(Y)}\n",
    " \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d609b1b",
   "metadata": {
    "editable": true
   },
   "source": [
    "Both, the nominator and denominator are now approximated numerically, whereas the variance (nominator) is defined with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87b83d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}(Y) = \\left(\\frac{1}{M-1} \\sum_{j=1}^M \\left(y_{\\mathbf{B}}^j\\right)^2\\right) - f_0^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083bc4a",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $f_0^2$ which is $\\left(\\mathbb{E}(Y)\\right)^2$.\n",
    "For $f_0^2$ exist several approximations, one common one is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c003d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "f_0^2 =  \\frac{1}{M^2} \\left(\\sum_{j=1}^M y_{\\mathbf{A}}^j \\right) \\left(  \\sum_{j=1}^M y_{\\mathbf{B}}^j \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647756d",
   "metadata": {
    "editable": true
   },
   "source": [
    "The conditional variance is approximated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fba49b",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}\\left(\\mathbb{E}(Y| Z_i) \\right) = \\frac{1}{M-1} \\sum_{j=1}^M y_{\\mathbf{A}}^j y_{\\mathbf{C_i}}^j - f_0^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfd79b0",
   "metadata": {
    "editable": true
   },
   "source": [
    "### The total indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91d5a6",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "S_{Ti} = \\frac{\\mathbb{E}\\left(\\operatorname{Var}(Y| \\mathbf{Z}_{-i}) \\right)}{\\operatorname{Var}(Y)} = 1 - \\frac{\\operatorname{Var}\\left(\\mathbb{E}(Y| \\mathbf{Z}_{-i}) \\right)}{\\operatorname{Var}(Y)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ecaf9",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here the variance is estimated accordingly, but taking the matrix A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5067bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}(Y) = \\left(\\frac{1}{M-1} \\sum_{j=1}^M \\left(y_{\\mathbf{A}}^j\\right)^2\\right) - f_0^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0d3eb",
   "metadata": {
    "editable": true
   },
   "source": [
    "here $f_0^2$ is approximated with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8381f5d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "f_0^2 =  \\frac{1}{M^2} \\left(\\sum_{j=1}^M y_{\\mathbf{A}}^j \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ee684",
   "metadata": {
    "editable": true
   },
   "source": [
    "And the conditional variance of not given $Z_i$ is approximated with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c9a16",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}\\left(\\mathbb{E}(Y)| \\mathbf{Z}_{-i} \\right) = \\left(\\frac{1}{M-1} \\sum_{j=1}^M y_{\\mathbf{B}}^j y_{\\mathbf{C_i}}^j\\right) - f_0^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af10987",
   "metadata": {
    "editable": true
   },
   "source": [
    "Those equations are implemented in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b345eddc",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# mc algorithm for variance based sensitivity coefficients\n",
    "def calculate_sensitivity_indices_mc(y_a, y_b, y_c, main='sobol', total='homma'):\n",
    "    \"\"\"\n",
    "    Sobol's 1993 algorithm for S_m using Monte Carlo integration\n",
    "    Sobol's 2007 algorithm for S_t using Monte Carlo integration\n",
    "    Saltelli's 2010 algorithm for estimating S_m and \n",
    "    Homma's 1996 algorithm for estimating S_t\n",
    "    \n",
    "    Inputs: y_a, y_b (array): first index corresponds to sample second to variables of interest\n",
    "    y_c (array): first index corresponds conditional index, second to sample and \n",
    "        following dimensions to variables of interest\n",
    "        main (str): specify which method from ('sobol', 'saltelli') to use for S_m\n",
    "        total (str): specify which method from ('sobol', 'homma') to use for S_t\n",
    "        \n",
    "    Returns: s, st\n",
    "        s (array): first order sensitivities first index corrresponds to input second \n",
    "            to variable of interest\n",
    "        st (array): total sensitivities first index corrresponds to input second \n",
    "            to variable of interest\n",
    "    \"\"\"\n",
    "    s_shape = y_c.shape[0:1] + y_c.shape[2:]\n",
    "    s = np.zeros(s_shape)\n",
    "    st = np.zeros(s_shape)\n",
    "\n",
    "    mean = 0.5*(np.mean(y_a,axis=0) + np.mean(y_b,axis=0))\n",
    "    y_a_center = y_a - mean\n",
    "    y_b_center = y_b - mean\n",
    "    f0sq = np.mean(y_a_center,axis=0) * np.mean(y_b_center,axis=0) # 0 when data is centered\n",
    "    var_est = np.var(y_b, axis=0)\n",
    "    for i, y_c_i in enumerate(y_c):\n",
    "        y_c_i_center = y_c_i - mean\n",
    "        if main=='sobol':\n",
    "            s[i] = (np.mean(y_a_center*y_c_i_center, axis=0)-f0sq)/var_est \n",
    "        elif main=='saltelli':\n",
    "            s[i] = np.mean(y_a_center*(y_c_i_center - y_b_center), axis=0)/var_est \n",
    "        else:\n",
    "            raise ValueError('Unknown method main=\"%s\"' % main)\n",
    "\n",
    "        if total=='homma':\n",
    "            st[i] = 1 - (np.mean(y_c_i_center*y_b_center, axis=0) - f0sq)/var_est \n",
    "        elif total=='sobol':\n",
    "            st[i] = np.mean(y_b_center*(y_b_center-y_c_i_center), axis=0)/var_est\n",
    "        else:\n",
    "            raise ValueError('Unknown method total=\"%s\"' % total)\n",
    "    return s, st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bfb572",
   "metadata": {
    "editable": true
   },
   "source": [
    "# References\n",
    "\n",
    "1. <div id=\"saltelli2010\"></div> **A. Saltelli, P. Annoni, I. Azzini, F. Campolongo, M. Ratto and S. Tarantola**.  Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index, *Computer Physics Communications*, 181(2), pp. 259-270, 2010."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
