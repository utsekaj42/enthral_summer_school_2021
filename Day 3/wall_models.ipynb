{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990082b2",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html wall_models.do.txt --ipynb_admon=hrule --without_solutions --no_abort -->\n",
    "<!-- dom:TITLE: Uncertainty quantification and sensitivity analysis for arterial wall models -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c84a61",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Uncertainty quantification and sensitivity analysis for arterial wall models\n",
    "**Vinzenz Gregor Eck**, Expert Analytics, Oslo  \n",
    "**Jacob Sturdy**, Department of Structural Engineering, NTNU\n",
    "\n",
    "Date: **Jun 30, 2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90cfded",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ipython magic\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc37f6d5",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e0d6e0",
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import chaospy as cp\n",
    "import chaospy_wrapper as cpw\n",
    "import numpy as np\n",
    "from monte_carlo import generate_sample_matrices_mc\n",
    "from monte_carlo import calculate_sensitivity_indices_mc\n",
    "unit_mmhg_pa = 133.3\n",
    "unit_pa_mmhg = 1./unit_mmhg_pa\n",
    "unit_cm2_m2 = 1. / 100. / 100.\n",
    "unit_m2_cm2 = 1. / unit_cm2_m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11ace7",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Introduction\n",
    "<div id=\"sec:introduction\"></div>\n",
    "The arterial wall models we are investigating in this part, are used to describe the (visco-)elastic behaviour of arteries in one-dimensional\n",
    "simulations of the cardiovascular system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92cb5d",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Arterial Wall Models\n",
    "<div id=\"sec:polynomialChaos\"></div>\n",
    "\n",
    "The elastic wall models are simplified algebraic functions $A(P)$ ([[eck2015stochastic;@boileau_benchmark_2015]](#eck2015stochastic;@boileau_benchmark_2015)),\n",
    "which state the arterial lumen area $A$ as function of transmural pressure $P$.\n",
    "\n",
    "For the calibration of the applied wall models, the wave speed in an arterial segment is required.\n",
    "The wave speed is given from fluid dynamics equations for one-dimensional arteries ([[eck2015stochastic]](#eck2015stochastic)):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f397ba",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:defwaveSpeed\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\tag{1}\n",
    "c(P) = \\sqrt{\\frac{A(P)}{\\rho\\ C(P)}},\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68599655",
   "metadata": {
    "editable": true
   },
   "source": [
    "with blood density $\\rho= 1050\\ [kg\\ m^{-3}]$ and compliance $C(P) = dA / dP$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63692670",
   "metadata": {
    "editable": true
   },
   "source": [
    "## *Quadratic* model\n",
    "The *Quadratic* area-pressure relationship ([[sherwin2003]](#sherwin2003)) is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37378ff",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:lapArea\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A(P) = \\left((P-P_s) \\frac{A_s}{\\lambda} + \\sqrt{A_s} \\right)^2,\n",
    " \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782d0dd",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\lambda$ is referred to as the stiffness coefficient and $A_s$\n",
    "is the area at the reference pressure $P_s$.\n",
    "\n",
    "The stiffness coefficient $\\lambda$ is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09500d4",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\lambda = \\frac{2 \\rho c_s^2 A_s}{\\sqrt{A_s}}\n",
    " \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20dc7d9",
   "metadata": {
    "editable": true
   },
   "source": [
    "## *Logarithmic* model\n",
    "The *Logarithmic* area-pressure relationship ([[Hayashi1980]](#Hayashi1980)) is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50596e59",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:expArea\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A(P) = A_s \\left( 1 + \\frac{1}{\\beta} ln \\left(\\frac{P}{P_s}\\right) \\right)^2,\n",
    " \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392a440a",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\beta$ is called the stiffness coefficient and $A_s$ is the\n",
    "area at the reference pressure $P_s$.\n",
    "\n",
    "The stiffness coefficient $\\beta$ is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec196f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\beta = \\frac{2\\ \\rho c_s^2}{P_s}\n",
    " \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4c18e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Implement these two models in Python\n",
    "\n",
    "Write a function that implements the relationship based on the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e65dc7",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def quadratic_area_model(pressure, parameters):\n",
    "    # Implement the function here\n",
    "    a_s, c_s, p_s, rho = parameters # Works when the first index of parameters indexes the parameters\n",
    "    area = 0 # You need to implement this\n",
    "    return area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cf188",
   "metadata": {
    "editable": true
   },
   "source": [
    "See the solution if you need help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f707818",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Follow the 6 (7) steps of UQSA for this model\n",
    "\n",
    "Perhaps you want to have a personalized or localized model of the arterial wall? How can you get values for $A_s$, $P_s$, $c$ and $\\rho$? How much uncertainty do you have about these values? Do you imagine a different use case for this model? What uncertainties might be present in that use case?\n",
    "\n",
    "* Step 1 Identification of the output(s) of interest Y\n",
    "\n",
    "* Step 2 Identify the inputs of interest and the appropriate distribution for the situation you are interested in? Can you support your choice of distribution? (If you prefer to move on just assume some nominal 10\\% uncertainty on the parameters. What does 10\\% uncertainty mean? If you model this with a Normal random variable or Uniform random variable does it have a different meaning?).\n",
    "\n",
    "* Step 3 Sampling of the input space to acquire samples of your inputs in a manner suited to your method for step 5\n",
    "\n",
    "* Step 4 Evaluate the deterministic model at each sample point to obtain your output samples\n",
    "\n",
    "* Step 5 Calculation of UQ and SA measures with your method of choice: Monte Carlo or Polynomial Chaos\n",
    "\n",
    "* Step 6 Assess the convergence of UQ and SA measures\n",
    "\n",
    "* Step 7 interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2a86e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3: Evaluate your external model\n",
    "\n",
    "If you have a model that is not written in python or needs to be evaluated offline. You can follow the same procedure with some minor modifications.\n",
    "\n",
    "For Step 3 you will need to write these values out your model of interest. Do you need to generate an input file for each? Can you load a list of parameter values from a csv file? In any case you can generate a data file containing the sample points and then determine how to run your model for each sample in the data points.\n",
    "\n",
    "For Step 4 you will use the output from step 3, and collect the output values in a similar data file.\n",
    "\n",
    "For Step 5 you will load the results file from Step 4 and then proceed as if the values had been generated in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6667af1e",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# example sampling\n",
    "u1 = cp.Uniform(0,1)\n",
    "u2 = cp.Uniform(0,1)\n",
    "joint_distribution = cp.J(u1, u2)\n",
    "number_of_samples = 350\n",
    "samples_random = joint_distribution.sample(size=number_of_samples, rule='R')\n",
    "# end example sample gen\n",
    "samples_hammersley = joint_distribution.sample(size=number_of_samples, rule='hammersley')\n",
    "samples_sobol = joint_distribution.sample(size=number_of_samples, rule='sobol')\n",
    "samples_lhs = joint_distribution.sample(size=number_of_samples, rule='latin_hypercube')\n",
    "samples_halton = joint_distribution.sample(size=number_of_samples, rule='halton')\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Random')\n",
    "ax1.scatter(*samples_random)\n",
    "ax1.set_xlabel(\"Uniform 1\")\n",
    "ax1.set_ylabel(\"Uniform 2\")\n",
    "ax1.axis('equal')\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Hammersley sampling')\n",
    "ax2.scatter(*samples_hammersley)\n",
    "ax2.set_xlabel(\"Uniform 1\")\n",
    "ax2.set_ylabel(\"Uniform 2\")\n",
    "ax2.axis('equal')\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Sobol sampling')\n",
    "ax2.scatter(*samples_sobol)\n",
    "ax2.set_xlabel(\"Uniform 1\")\n",
    "ax2.set_ylabel(\"Uniform 2\")\n",
    "ax2.axis('equal')\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('Latin Hypercube sampling')\n",
    "ax2.scatter(*samples_lhs)\n",
    "ax2.set_xlabel(\"Uniform 1\")\n",
    "ax2.set_ylabel(\"Uniform 2\")\n",
    "_ = ax2.axis('equal')\n",
    "# end example sampling\n",
    "\n",
    "# example save samples to file\n",
    "# Creates a csv file where each row corresponds to the sample number and each column with teh variables in the joint distribution\n",
    "csv_file = \"csv_samples.csv\"\n",
    "sep = '\\t'\n",
    "header = [\"u1\", \"u2\"]\n",
    "header = sep.join(header)\n",
    "np.savetxt(csv_file, samples_random, delimiter=sep, header=header)\n",
    "# end example save samples to file\n",
    "\n",
    "# generate external data\n",
    "# load external samples\n",
    "samples_random = np.genfromtxt(csv_file)\n",
    "\n",
    "# evaluate model\n",
    "ext_data = np.array([sample[0] + sample[1] + sample[0]*sample[1] for sample in samples_random.T])\n",
    "header = ['y0']\n",
    "header = sep.join(header)\n",
    "filepath = \"external_evaluations.csv\"\n",
    "np.savetxt(filepath, ext_data, delimiter=sep, header=header)\n",
    "# end generate external data\n",
    "\n",
    "# example load samples from file\n",
    "# loads a csv file where the samples/or model evaluations for each sample are saved\n",
    "# with one sample per row. Multiple components ofoutput can be stored as separate columns \n",
    "filepath = \"external_evaluations.csv\"\n",
    "data = np.genfromtxt(filepath)\n",
    "# end example load samples from file\n",
    "\n",
    "# === quadrature ===\n",
    "# quadrature in polychaos\n",
    "#cp.generate_quadrature?\n",
    "# end quadrature in polychaos\n",
    "\n",
    "# example quadrature\n",
    "u1 = cp.Uniform(0,1)\n",
    "u2 = cp.Uniform(0,1)\n",
    "joint_distribution = cp.J(u1, u2)\n",
    "\n",
    "order = 5\n",
    "\n",
    "nodes_gaussian, weights_gaussian = cp.generate_quadrature(order, joint_distribution, rule='G')\n",
    "nodes_clenshaw, weights_clenshaw = cp.generate_quadrature(order, joint_distribution, rule='C')\n",
    "\n",
    "print('Number of nodes gaussian quadrature: {}'.format(len(nodes_gaussian[0])))\n",
    "print('Number of nodes clenshaw-curtis quadrature: {}'.format(len(nodes_clenshaw[1])))\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.scatter(*nodes_gaussian, marker='o', color='b')\n",
    "ax1.scatter(*nodes_clenshaw, marker= 'x', color='r')\n",
    "ax1.set_xlabel(\"Uniform 1\")\n",
    "ax1.set_ylabel(\"Uniform 2\")\n",
    "ax1.axis('equal')\n",
    "# end example quadrature\n",
    "\n",
    "# example sparse grid quadrature\n",
    "u1 = cp.Uniform(0,1)\n",
    "joint_distribution = cp.Iid(u1, 5)\n",
    "\n",
    "order = 2\n",
    "# sparse grid has exponential growth, thus a smaller order results in more points\n",
    "nodes_clenshaw, weights_clenshaw = cp.generate_quadrature(order, joint_distribution, rule='C')\n",
    "nodes_clenshaw_sparse, weights_clenshaw_sparse = cp.generate_quadrature(order, joint_distribution, rule='C', sparse=True)\n",
    "\n",
    "print('Number of nodes normal clenshaw-curtis quadrature: {}'.format(len(nodes_clenshaw[0])))\n",
    "print('Number of nodes clenshaw-curtis quadrature with sparse grid : {}'.format(len(nodes_clenshaw_sparse[0])))\n",
    "\n",
    "#fig1, ax1 = plt.subplots()\n",
    "#ax1.scatter(*nodes_clenshaw, marker= 'x')\n",
    "#ax1.scatter(*nodes_clenshaw_sparse, marker= 's')\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.scatter(nodes_clenshaw_sparse[0], nodes_clenshaw_sparse[1], marker= '^')\n",
    "ax1.scatter(nodes_clenshaw[0],nodes_clenshaw[1], marker= 'x')\n",
    "ax1.set_xlabel(\"Uniform 1\")\n",
    "ax1.set_ylabel(\"Uniform 2\")\n",
    "ax1.axis('equal')\n",
    "# end example sparse grid quadrature\n",
    "\n",
    "# example orthogonalization schemes\n",
    "# a normal random variable\n",
    "n = cp.Normal(0, 1)\n",
    "\n",
    "x = np.linspace(0,1, 50)\n",
    "# the polynomial order of the orthogonal polynomials\n",
    "polynomial_order = 3\n",
    "\n",
    "poly = cp.generate_expansion(polynomial_order, n, rule='cholesky', normed=True)\n",
    "print('Cholesky decomposition {}'.format(poly))\n",
    "ax = plt.subplot(131)\n",
    "ax.set_title('Cholesky decomposition')\n",
    "_=plt.plot(x, poly(x).T)\n",
    "_=plt.xticks([])\n",
    "\n",
    "poly = cp.generate_expansion(polynomial_order, n, rule='ttr', normed=True)\n",
    "print('Discretized Stieltjes / Three terms reccursion {}'.format(poly))\n",
    "ax = plt.subplot(132)\n",
    "ax.set_title('Discretized Stieltjes ')\n",
    "_=plt.plot(x, poly(x).T)\n",
    "\n",
    "# TODO: this is broken\n",
    "#poly = cp.generate_expansion(polynomial_order, n, rule='gram_schmidt', normed=True)\n",
    "#print('Modified Gram-Schmidt {}'.format(poly))\n",
    "#ax = plt.subplot(133)\n",
    "#ax.set_title('Modified Gram-Schmidt')\n",
    "#_=plt.plot(x, poly(x).T)\n",
    "# end example orthogonalization schemes\n",
    "\n",
    "# _Linear Regression_\n",
    "# linear regression in chaospy\n",
    "#cp.fit_regression?\n",
    "# end linear regression in chaospy\n",
    "\n",
    "\n",
    "# example linear regression\n",
    "# 1. define marginal and joint distributions\n",
    "u1 = cp.Uniform(0,1)\n",
    "u2 = cp.Uniform(0,1)\n",
    "joint_distribution = cp.J(u1, u2)\n",
    "\n",
    "# 2. generate orthogonal polynomials\n",
    "polynomial_order = 3\n",
    "poly = cp.generate_expansion(polynomial_order, joint_distribution)\n",
    "\n",
    "# 3.1 generate samples\n",
    "number_of_samples = len(poly) \n",
    "samples = joint_distribution.sample(size=number_of_samples, rule='R')\n",
    "\n",
    "# 3.2 evaluate the simple model for all samples\n",
    "model_evaluations = samples[0]+samples[1]*samples[0]\n",
    "\n",
    "# 3.3 use regression to generate the polynomial chaos expansion\n",
    "gpce_regression = cp.fit_regression(poly, samples, model_evaluations)\n",
    "# end example linear regression\n",
    "\n",
    "\n",
    "# _Spectral Projection_\n",
    "# spectral projection in chaospy\n",
    "# cp.fit_quadrature?\n",
    "# end spectral projection in chaospy\n",
    "\n",
    "\n",
    "# example spectral projection\n",
    "# 1. define marginal and joint distributions\n",
    "u1 = cp.Uniform(0,1)\n",
    "u2 = cp.Uniform(0,1)\n",
    "joint_distribution = cp.J(u1, u2)\n",
    "\n",
    "# 2. generate orthogonal polynomials\n",
    "polynomial_order = 3\n",
    "poly = cp.generate_expansion(polynomial_order, joint_distribution)\n",
    "\n",
    "# 4.1 generate quadrature nodes and weights\n",
    "order = 5\n",
    "nodes, weights = cp.generate_quadrature(order, joint_distribution, rule='G')\n",
    "\n",
    "# 4.2 evaluate the simple model for all nodes\n",
    "model_evaluations = nodes[0]+nodes[1]*nodes[0]\n",
    "\n",
    "# 4.3 use quadrature to generate the polynomial chaos expansion\n",
    "gpce_quadrature = cp.fit_quadrature(poly, nodes, weights, model_evaluations)\n",
    "print(\"Success\")\n",
    "# end example spectral projection\n",
    "\n",
    "# example uq\n",
    "exp_reg = cp.E(gpce_regression, joint_distribution)\n",
    "exp_ps =  cp.E(gpce_quadrature, joint_distribution)\n",
    "\n",
    "std_reg = cp.Std(gpce_regression, joint_distribution)\n",
    "str_ps = cp.Std(gpce_quadrature, joint_distribution)\n",
    "\n",
    "prediction_interval_reg = cp.Perc(gpce_regression, [5, 95], joint_distribution)\n",
    "prediction_interval_ps = cp.Perc(gpce_quadrature, [5, 95], joint_distribution)\n",
    "\n",
    "print(\"Expected values   Standard deviation            90 % Prediction intervals\\n\")\n",
    "print(' E_reg |  E_ps     std_reg |  std_ps                pred_reg |  pred_ps')\n",
    "print('  {} | {}       {:>6.3f} | {:>6.3f}       {} | {}'.format(exp_reg,\n",
    "                                                                  exp_ps,\n",
    "                                                                  std_reg,\n",
    "                                                                  str_ps,\n",
    "                                                                  [\"{:.3f}\".format(p) for p in prediction_interval_reg],\n",
    "                                                                  [\"{:.3f}\".format(p) for p in prediction_interval_ps]))\n",
    "# end example uq\n",
    "\n",
    "# example sens\n",
    "sensFirst_reg = cp.Sens_m(gpce_regression, joint_distribution)\n",
    "sensFirst_ps = cp.Sens_m(gpce_quadrature, joint_distribution)\n",
    "\n",
    "sensT_reg = cp.Sens_t(gpce_regression, joint_distribution)\n",
    "sensT_ps = cp.Sens_t(gpce_quadrature, joint_distribution)\n",
    "\n",
    "print(\"First Order Indices           Total Sensitivity Indices\\n\")\n",
    "print('       S_reg |  S_ps                 ST_reg |  ST_ps  \\n')\n",
    "for k, (s_reg, s_ps, st_reg, st_ps) in enumerate(zip(sensFirst_reg, sensFirst_ps, sensT_reg, sensT_ps)):\n",
    "    print('S_{} : {:>6.3f} | {:>6.3f}         ST_{} : {:>6.3f} | {:>6.3f}'.format(k, s_reg, s_ps, k, st_reg, st_ps))\n",
    "# end example sens\n",
    "\n",
    "# example exact solution\n",
    "import sympy as sp\n",
    "import sympy.stats\n",
    "from sympy.utilities.lambdify import lambdify, implemented_function\n",
    "\n",
    "pdf_beta = lambda b: 1\n",
    "support_beta = (pdf_beta,0,1)\n",
    "         \n",
    "pdf_chi = lambda x:  1\n",
    "support_chi = (pdf_chi,0, 1)\n",
    "x, b = sp.symbols(\"x, b\")\n",
    "y = x + x*b\n",
    "\n",
    "support_beta = (b,0,1)\n",
    "support_chi = (x,0,1)\n",
    "mean_g_beta = sp.Integral(y*pdf_chi(x), support_chi)\n",
    "mean_g_chi =  sp.Integral(y*pdf_beta(b), support_beta)\n",
    "mean = sp.Integral(mean_g_beta*pdf_beta(b), support_beta)\n",
    "print(\"Expected value {}\".format(mean.doit()))\n",
    "variance = sp.Integral(pdf_beta(b)*sp.Integral(pdf_chi(x)*(y-mean)**2,support_chi), support_beta)\n",
    "print(\"Variance: {}\".format(variance.doit()))\n",
    "var_E_g_beta = sp.Integral(pdf_beta(b)*(mean_g_beta-mean)**2, support_beta)\n",
    "var_E_g_chi = sp.Integral(pdf_chi(x)*(mean_g_chi-mean)**2, support_chi)\n",
    "\n",
    "S_chi =  var_E_g_chi/variance\n",
    "S_beta = var_E_g_beta/variance\n",
    "\n",
    "\n",
    "print(\"S_beta {}\".format(S_beta.doit()))\n",
    "print(\"S_chi {}\".format(S_chi.doit()))\n",
    "# end example exact solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d16a16c",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# example save samples to file\n",
    "# Creates a csv file where each row corresponds to the sample number and each column with teh variables in the joint distribution\n",
    "csv_file = \"csv_samples.csv\"\n",
    "sep = '\\t'\n",
    "header = [\"u1\", \"u2\"]\n",
    "header = sep.join(header)\n",
    "np.savetxt(csv_file, samples_random, delimiter=sep, header=header)\n",
    "# end example save samples to file\n",
    "\n",
    "# generate external data\n",
    "# load external samples\n",
    "samples_random = np.genfromtxt(csv_file)\n",
    "\n",
    "# evaluate model\n",
    "ext_data = np.array([sample[0] + sample[1] + sample[0]*sample[1] for sample in samples_random.T])\n",
    "header = ['y0']\n",
    "header = sep.join(header)\n",
    "filepath = \"external_evaluations.csv\"\n",
    "np.savetxt(filepath, ext_data, delimiter=sep, header=header)\n",
    "# end generate external data\n",
    "\n",
    "# example load samples from file\n",
    "# loads a csv file where the samples/or model evaluations for each sample are saved\n",
    "# with one sample per row. Multiple components ofoutput can be stored as separate columns \n",
    "filepath = \"external_evaluations.csv\"\n",
    "data = np.genfromtxt(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e8b05",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. <div id=\"eck2015stochastic\"></div> **V. G. Eck, J. Feinberg, H. P. Langtangen and L. R. Hellevik**.  Stochastic Sensitivity Analysis for Timing and Amplitude of Pressure waves in the Arterial System, *International Journal for Numerical Methods in Biomedical Engineering*, 31(4), 2015.\n",
    "\n",
    "2. <div id=\"boileau_benchmark_2015\"></div> **E. Boileau, P. Nithiarasu, P. J. Blanco, L. O. Mueller, F. E. Fossan, L. R. Hellevik, W. P. Donders, W. Huberts, M. Willemet and J. Alastruey**.  A benchmark study of numerical schemes for one-dimensional arterial blood flow modelling, *International Journal for Numerical Methods in Biomedical Engineering*, 31(10), pp. n/a-n/a, 2015.\n",
    "\n",
    "3. <div id=\"sherwin2003\"></div> **S. Sherwin, V. Franke, J. Peir\\'o and K. Parker**.  One-Dimensional Modelling of Vascular Network in Space-Time Variables, *Journal of Engineering Mathematics*, 47(3-4), pp. 217-233, 2003.\n",
    "\n",
    "4. <div id=\"Hayashi1980\"></div> **K. Hayashi, H. Handa, S. Nagasawa, A. Okumura and K. Moritake**.  Stiffness and Elastic Behavior of Human Intracranial and Extracranial arteries, *Journal of Biomechanics*, 13(2), pp. 175-184, 1980."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
